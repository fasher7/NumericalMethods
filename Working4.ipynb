{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** Fahim Shahriar Eram\n",
        "\n",
        "**ID:** 2022523"
      ],
      "metadata": {
        "id": "WtMGCiggVwGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Polynomial Regression**\n",
        "\n",
        "In this assignment, you will implement polynomial regression and apply it to the [Assignment 4 Dataset](https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/mirsazzathossain/CSE317-Lab-Numerical-Methods/blob/main/datasets/data.csv).\n",
        "\n",
        "The dataset contains two columns, the first column is the feature and the second column is the label. The goal is find the best fit line for the data.\n",
        "\n",
        "You will need to perform the following regression tasks and find the best one for the dataset.\n",
        "\n",
        "1.    **Linear Regression:**\n",
        "\n",
        "     The equation we are trying to fit is:\n",
        "     $$y = \\theta_0 + \\theta_1 x$$\n",
        "     where $x$ is the feature and $y$ is the label.\n",
        "\n",
        "     We can rewrite the equation in vector form as:\n",
        "$$Y = X\\theta$$ where $X$ is a matrix with two columns, the first column is all 1s and the second column is the feature, and $Y$ is a vector with the labels. $\\theta$ is a vector with two elements, $\\theta_0$ and $\\theta_1$. The $X$ matrix will look like this:\n",
        "$$X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}$$\n",
        "2. **Quadratic Regression:**\n",
        "\n",
        "     The equation we are trying to fit is:\n",
        "     $$y = \\theta_0 + \\theta_1 x + \\theta_2 x^2$$\n",
        "     where $x$ is the feature and $y$ is the label.\n",
        "\n",
        "     We can rewrite the equation in vector form as:\n",
        "$$Y = X\\theta$$where $X$ is a matrix with three columns, the first column is all 1s, the second column is the feature, and the third column is the feature squared, and $Y$ is a vector with the labels. $\\theta$ is a vector with three elements, $\\theta_0$, $\\theta_1$, and $\\theta_2$. The $X$ matrix will look like this:\n",
        "\n",
        "$$X = \\begin{bmatrix} 1 & x_1 & x_1^2 \\\\ 1 & x_2 & x_2^2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 \\end{bmatrix}$$\n",
        "3. **Cubic Regression:**\n",
        "\n",
        "     The equation we are trying to fit is:\n",
        "$$y = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + \\theta_3 x^3$$\n",
        "     where $x$ is the feature and $y$ is the label.\n",
        "\n",
        "     We can rewrite the equation in vector form as:\n",
        "$$Y = X\\theta$$where $X$ is a matrix with four columns, the first column is all 1s, the second column is the feature, the third column is the feature squared, and the fourth column is the feature cubed, and $Y$ is a vector with the labels. $\\theta$ is a vector with four elements, $\\theta_0$, $\\theta_1$, $\\theta_2$, and $\\theta_3$. The $X$ matrix will look like this:\n",
        "$$X = \\begin{bmatrix} 1 & x_1 & x_1^2 & x_1^3 \\\\ 1 & x_2 & x_2^2 & x_2^3 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_n & x_n^2 & x_n^3 \\end{bmatrix}$$\n",
        "\n",
        "Take 15 data points from the dataset and use them as the training set. Use the remaining data points as the test set. For each regression task, find the best $\\theta$ vector using the training set. Then, calculate the mean squared error (MSE) on the test set. Plot the training set, the test set (in a different color), and the best fit line for each regression task. Which regression task gives the best fit line? Which regression task gives the lowest MSE on the test set? Report your answers in a Markdown cell.\n",
        "\n",
        "**Note:** Do not use any built-in functions like `np.polyfit` or `sklearn.linear_model.LinearRegression` or any other built-in functions that perform polynomial regression. You must implement the regression tasks yourself."
      ],
      "metadata": {
        "id": "kAPQiFOHL9aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary packages**"
      ],
      "metadata": {
        "id": "wP2MDC7mWzxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "u1L_7nhbWpn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Uploading Dataset*"
      ],
      "metadata": {
        "id": "92k0ymwKXuZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/data.csv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjSLa3mTXMSP",
        "outputId": "b763610a-354b-4165-a59e-a01ff411d2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data.csv.zip\n",
            "replace data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Loading Dataset*"
      ],
      "metadata": {
        "id": "09TS2OosX9Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myData = pd.read_csv(\"data.csv\", header=None)\n",
        "myData.columns = [\"Feature\", \"Label\"]"
      ],
      "metadata": {
        "id": "rcspns3gX7fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Seperating Data*"
      ],
      "metadata": {
        "id": "N216p-Xkk9H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = myData[\"Feature\"]\n",
        "label = myData[\"Label\"]\n",
        "\n",
        "oneMatrix = np.ones((20, 1))\n",
        "X = np.vstack(features.to_numpy())\n",
        "X = np.append(oneMatrix, X, axis=1)\n",
        "Y = np.vstack(label.to_numpy())"
      ],
      "metadata": {
        "id": "3IEOv-UPZ11A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Splitting Data*"
      ],
      "metadata": {
        "id": "SPyEWocJuf7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_X, test_X = np.split(X, [int(0.75 * len(X))], axis = 0)\n",
        "train_Y, test_Y = np.split(Y, [int(0.75 * len(Y))], axis = 0)"
      ],
      "metadata": {
        "id": "OONz0I7IujN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Accuracy of Linear Regression*"
      ],
      "metadata": {
        "id": "y4KGdsLwt1qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.matmul(np.transpose(train_X), train_X)\n",
        "R_INV = np.linalg.inv(R)\n",
        "Q = train_X\n",
        "Q_T = np.transpose(Q)\n",
        "beta1 = np.matmul(R_INV, Q_T).dot(train_Y)\n",
        "\n",
        "R = np.matmul(np.transpose(test_X), test_X)\n",
        "R_INV = np.linalg.inv(R)\n",
        "Q = test_X\n",
        "Q_T = np.transpose(Q)\n",
        "beta2 = np.matmul(R_INV, Q_T).dot(test_Y)\n",
        "\n",
        "MSE_Linear = round(np.square(np.subtract(beta1,beta2)).mean(), 3)\n",
        "\n",
        "print(\"{0}%\".format(MSE_Linear))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kelappXEt8_O",
        "outputId": "e6c291f8-4853-4c87-d9cf-80aa64baa030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38.287%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Plotting Linear Regression*"
      ],
      "metadata": {
        "id": "BY7h-cJI7Mj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_axis1 = train_X[:,1]\n",
        "# x_axis2 = test_X[:,1]\n",
        "\n",
        "# plt.scatter(x_axis1, train_Y, color = \"purple\")\n",
        "# plt.scatter(x_axis2, test_Y, color = \"green\")\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "3E6G8Tbw7ShF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Accuracy of Quadratic Regression*"
      ],
      "metadata": {
        "id": "wkbBgeRZU1ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quadratic_train = np.append(train_X, np.vstack(train_X[:,1]**2), axis=1)\n",
        "quadratic_test =  np.append(test_X, np.vstack(test_X[:,1]**2), axis=1)\n",
        "\n",
        "R = np.matmul(np.transpose(quadratic_train), quadratic_train)\n",
        "R_INV = np.linalg.inv(R)\n",
        "Q = quadratic_train\n",
        "Q_T = np.transpose(Q)\n",
        "beta3 = np.matmul(R_INV, Q_T).dot(train_Y)\n",
        "\n",
        "R = np.matmul(np.transpose(quadratic_test), quadratic_test)\n",
        "R_INV = np.linalg.inv(R)\n",
        "Q = quadratic_test\n",
        "Q_T = np.transpose(Q)\n",
        "beta4 = np.matmul(R_INV, Q_T).dot(test_Y)\n",
        "\n",
        "MSE_Quadratic = round(np.square(np.subtract(beta3,beta4)).mean(), 3)\n",
        "\n",
        "print(\"{0}%\".format(MSE_Quadratic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwvHxtd2U-um",
        "outputId": "c8474a41-a4e1-4608-bf8a-4a11224bb563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11.206%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Plotting of Quadratic Regression*"
      ],
      "metadata": {
        "id": "hK7VyCotU_J7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQ-fO5lSVDQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Accuracy of Cubic Regression*"
      ],
      "metadata": {
        "id": "17ouCg3YVDvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cubic_train = np.append(quadratic_train, np.vstack(train_X[:,1]**3), axis=1)\n",
        "cubic_test =  np.append(quadratic_test, np.vstack(test_X[:,1]**3), axis=1)\n",
        "\n",
        "R = np.matmul(np.transpose(cubic_train), cubic_train)\n",
        "R_INV = np.linalg.inv(R)\n",
        "Q = cubic_train\n",
        "Q_T = np.transpose(Q)\n",
        "beta5 = np.matmul(R_INV, Q_T).dot(train_Y)\n",
        "\n",
        "R = np.matmul(np.transpose(cubic_test), cubic_test)\n",
        "R_INV = np.linalg.inv(R)\n",
        "Q = cubic_test\n",
        "Q_T = np.transpose(Q)\n",
        "beta6 = np.matmul(R_INV, Q_T).dot(test_Y)\n",
        "\n",
        "MSE_Cubic = round(np.square(np.subtract(beta5,beta6)).mean(), 3)\n",
        "\n",
        "print(\"{0}%\".format(MSE_Cubic))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF2gX15WVJg4",
        "outputId": "e5b05a92-0bb4-440b-80fb-f2ae8bfdb14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.206%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Plotting of Cubic Regression*"
      ],
      "metadata": {
        "id": "e1dRvk2KVKSL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cOfDdEI7b9IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "-\n",
        "\n",
        "-The cubic regression task gives the lowest MSE (4.206%) on the test set, which is far less than the MSE of quadratic regresion task (11.206%) and the MSE of linear regresstion task (38.287%)."
      ],
      "metadata": {
        "id": "XQiqMRigajHf"
      }
    }
  ]
}