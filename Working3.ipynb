{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** Fahim Shahriar Eram\n",
        "\n",
        "**ID:** 2022523"
      ],
      "metadata": {
        "id": "_bUx3p9WTmSy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RJljra5hVWR"
      },
      "source": [
        "#### **Wheat Seed Classification**\n",
        "\n",
        "In this assignment, you will use the [Wheat Seed Dataset](https://archive.ics.uci.edu/ml/datasets/seeds) to classify the type of wheat seed based on the measurements of the seed. The dataset contains 7 attributes and 210 instances. The attributes are:\n",
        "\n",
        "1. Area\n",
        "2. Perimeter\n",
        "3. Compactness\n",
        "4. Length of Kernel\n",
        "5. Width of Kernel\n",
        "6. Asymmetry Coefficient\n",
        "7. Length of Kernel Groove\n",
        "\n",
        "Based on the attributes, the dataset contains 3 classes:\n",
        "\n",
        "1. Kama\n",
        "2. Rosa\n",
        "3. Canadian\n",
        "\n",
        "The text file `seeds_dataset.txt` contains the dataset. The first 7 columns are the attributes and the last column is the class label. The class labels are encoded as  1, 2, and 3 for Kama, Rosa, and Canadian, respectively. The goal of this assignment is to build a classifier that can predict the type of wheat seed based on the measurements of the seed. Follow the instructions below to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Necessary Packages**"
      ],
      "metadata": {
        "id": "J-sCwSFWWQwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Solutions are given after Instructions"
      ],
      "metadata": {
        "id": "qefxcPEaWH4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVYEX8-ihVWS"
      },
      "source": [
        "#### **Instructions**\n",
        "\n",
        "1. Download the dataset from [Github](https://minhaskamal.github.io/DownGit/#/home?url=https://github.com/mirsazzathossain/CSE317-Lab-Numerical-Methods/blob/main/datasets/seeds_dataset.txt). It should be saved as `seeds_dataset.txt`.\n",
        "2. Upload the dataset to your Google Drive and mount your Google Drive to Colab.\n",
        "3. Read the dataset using numpy's built-in function `np.genfromtxt()`. Pass the following parameters to the function:\n",
        "    - `fname`: The path to the dataset\n",
        "    - `delimiter`: The delimiter used in the dataset to separate the attributes (Hint: Use `'\\t'` as the delimiter)\n",
        "    \n",
        "4. Shuffle the dataset using `np.random.shuffle()`. Pass the following parameters to the function:\n",
        "    - `x`: The dataset\n",
        "5. Split the dataset into features and labels. The first 7 columns of the dataset are the features and the last column is the label. Use numpy's array slicing to split the dataset into features and labels. (Hint: Use `:` to select all the rows and `0:7` to select the first 7 columns for features and `7` to select the last column for labels)\n",
        "6. Split the dataset into training and testing sets. Use numpy's built-in function `np.split()` to split the dataset into training and testing sets. Pass the following parameters to the function:\n",
        "    - `ary`: The dataset\n",
        "    - `indices_or_sections`: The number of instances in the training set (Hint: Use `int(0.8 * len(dataset))` to get the number of instances in the training set)\n",
        "    - `axis`: The axis to split the dataset (Hint: Use `0` to split the dataset along the rows)\n",
        "7. Find the minimum and maximum values of each feature in the training set. Use numpy's built-in function `np.min()` and `np.max()` to find the minimum and maximum values of each feature in the training set. Pass the following parameters to the function:\n",
        "    - `a`: The training set\n",
        "    - `axis`: The axis to find the minimum and maximum values (Hint: Use `0` to find the minimum and maximum values along the columns)\n",
        "8. In this step, you must normalize the training and test sets. Nomalization is an essential part of every machine learning project. It is used to bring all the features to the same scale. If the features are not normalized, the higher-valued features will outnumber the lower-valued ones.\n",
        "\n",
        "    For example, suppose we have a dataset with two features: the number of bedrooms in a house and the size of the garden in square feet and we are trying to forecast the rent of the residence. If the features are not normalized, the feature with higher values will take precedence over the feature with lower values. In this scenario, the garden area has a greater value. As a result, the model will make an attempt to forecast the house's price depending on the size of the garden. As a consequence, the model will be faulty since most individuals will not pay higher rent for more garden area. We need to normalize the features in order to prevent this. Let's look at the following illustration to better comprehend what we have said:\n",
        "    \n",
        "    - House 1: 2 bedrooms, 2500 sq. ft. garden\n",
        "    - House 2: 3 bedrooms, 500 sq. ft. garden\n",
        "    - House 3: 7 bedrooms, 2300 sq. ft. garden\n",
        "\n",
        "    Considering that most people won't pay more for a larger garden, it follows that the rent for House 1 should be more comparable to House 2 than to House 3. However, if we give the aforementioned data to a k-NN classifier without normalization, it will compute the euclidean distance between the test and training examples and pick the class of the test instance based on the class of the closest training instance.\n",
        "\n",
        "    The euclidean distance between the test instance and the training instances will be:\n",
        "\n",
        "    - Distance between house 1 and house 2: $\\sqrt{(2-3)^2 + (2500-500)^2} = 2000$\n",
        "    - Distance between house 1 and house 3: $\\sqrt{(2-7)^2 + (2500-2300)^2} = 200$\n",
        "\n",
        "    As you can see, the distance between houses 1 and 3 is shorter than that between houses 1 and 2. As a result, the model will forecast that house 1 will cost around the same as house 3. This is not what was anticipated. We need to normalize the features in order to prevent this. To normalize the features, subtract the minimum value of each feature from all the values of that feature and divide the result by the range of the feature. The range of a feature is the difference between the maximum and minimum values of that feature. The formula for normalization is given below:\n",
        "\n",
        "    $$x_{normalized} = \\frac{x - min(x)}{max(x) - min(x)}$$\n",
        "\n",
        "    where $x$ is the feature vector. The above formula will normalize the features to a scale of 0 to 1.\n",
        "\n",
        "    Let's normalize the features in the above example. To do so, we need to find the minimum and maximum values of each feature. The minimum and maximum values of the number of bedrooms are 2 and 7, respectively. The minimum and maximum values of the garden area are 500 and 2500, respectively. The normalized values of the features are given below:\n",
        "\n",
        "    - House 1: $(2 - 2) / 5 = 0$ bedrooms, $(2500 - 500) / 2000 = 0.75$ sq. ft. garden\n",
        "    - House 2: $(3 - 2) / 5 = 0.2$ bedrooms, $(500 - 500) / 2000 = 0$ sq. ft. garden\n",
        "    - House 3: $(7 - 2) / 5 = 1$ bedrooms, $(2300 - 500) / 2000 = 0.85$ sq. ft. garden\n",
        "\n",
        "    Now, the euclidean distance between the test instance and the training instances will be:\n",
        "\n",
        "    - Distance between house 1 and house 2: $\\sqrt{(0-0.2)^2 + (0.75-0)^2} = 0.77$\n",
        "    - Distance between house 1 and house 3: $\\sqrt{(0-1)^2 + (0.75-0.9)^2} = 1.11$\n",
        "\n",
        "    As you can see now, the distance between houses 1 and 2 is shorter than that between houses 1 and 3. The model will thus forecast that house 1 will cost about the same as house 2, according to the prediction. This is what is anticipated. This is what normalization does. It equalizes the scale of all features. This is important because it prevents the features with higher values from dominating the features with lower values.\n",
        "\n",
        "    Use the minimum and maximum values you found in the previous step to normalize the training and test sets.\n",
        "9. Now, you have to build a classifier to classify the type of wheat seed based on the measurements of the seed. Use the K-Nearest Neighbors algorithm to build the classifier. Use the Euclidean distance to find the nearest neighbors.\n",
        "\n",
        "10. Output the number of data points in the testing set and the number of correct predictions made by the classifier for each class."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "dataset_path = \"/content/drive/MyDrive/seeds_dataset.txt\""
      ],
      "metadata": {
        "id": "7wHspI8HX31N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681a8354-0335-4905-fdf0-f5844a64f7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = np.genfromtxt(fname = dataset_path, delimiter = \"\\t\")"
      ],
      "metadata": {
        "id": "a_HrpV4gcATw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(dataset)"
      ],
      "metadata": {
        "id": "iyhBIFI8mbao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = dataset[:, 0:7]\n",
        "labels = dataset[:, 7]"
      ],
      "metadata": {
        "id": "cTATfHYHoX4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, test_features = np.split(features, [int(0.8 * len(features))], axis = 0) \n",
        "train_labels, test_labels = np.split(labels, [int(0.8 * len(labels))], axis = 0) "
      ],
      "metadata": {
        "id": "jZx8ngmCPjZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minTrFeature = np.min(train_features, 0)\n",
        "maxTrFeature = np.max(train_features, 0)"
      ],
      "metadata": {
        "id": "cuF5Fzq4qYAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minTeFeature = np.min(test_features, 0)\n",
        "maxTeFeature = np.max(test_features, 0)\n",
        "\n",
        "normTrFeature = (train_features - minTrFeature) / (maxTrFeature - minTrFeature)\n",
        "normTeFeature = (test_features - minTeFeature) / (maxTeFeature - minTeFeature)"
      ],
      "metadata": {
        "id": "MKUHbBQStU4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = KNeighborsClassifier(n_neighbors=3)\n",
        "classifier.fit(train_features, train_labels)\n",
        "predictions = classifier.predict(test_features)\n",
        "\n",
        "def euclidean_distance(features, train_features):\n",
        "    euc_distances1 = np.sqrt(np.sum((features - train_features)**2, axis=1))\n",
        "    return euc_distances1\n",
        "def nearest_neighbor(features, train_features, train_labels):\n",
        "    distances = euclidean_distance(features, train_features)\n",
        "    indices = np.argsort(distances)\n",
        "    nearest_neighbor = train_labels[indices[0]]\n",
        "    return nearest_neighbor\n",
        "\n",
        "predicted_label = nearest_neighbor(normTeFeature[7], normTrFeature, train_labels)\n",
        "\n",
        "if(predicted_label==1):\n",
        "  print(\"Predicted: Kama\")\n",
        "elif(predicted_label==2):\n",
        "  print(\"Predicted: Rosa\")\n",
        "else:\n",
        "  print(\"Predicted: Canadian\")\n",
        "    \n",
        "if(test_labels[7]==1):\n",
        "  print(\"Actual label: Kama\")\n",
        "elif(test_labels[7]==2):\n",
        "  print(\"Actual label: Rosa\")\n",
        "else:\n",
        "  print(\"Actual label: Canadian\")"
      ],
      "metadata": {
        "id": "cQkFViif1kra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ce8ddf-d1b6-4775-af1f-5cfe53718a8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Rosa\n",
            "Actual label: Rosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(predictions, labels):\n",
        "    correct_predictions = np.sum(predictions == labels)\n",
        "    accuracy = round((correct_predictions / len(labels))*100, 3)\n",
        "    return accuracy, correct_predictions\n",
        "\n",
        "score, correct = accuracy(predictions, test_labels)\n",
        "print(\"Accuracy: {0}%\".format(score))\n",
        "print(\"Number of data points: {0}\".format(len(test_labels)))\n",
        "print(\"Correct predictions: {0}\".format(correct))"
      ],
      "metadata": {
        "id": "TTqWwOQYEDcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774aad90-6f07-467b-a610-326eeb9f8ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 90.476%\n",
            "Number of data points: 42\n",
            "Correct predictions: 38\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b148fc9bfa8b60132af830e32e1690e4e023b803e92912df15b823b90141dda6"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}